{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdf1c7-d7a1-4d6b-a38b-6d8cdd6183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the saved model and move it to the GPU or CPU\n",
    "model = torch.load('C:/Users/Bruss/Desktop/Speciale/models/deployments/finalmodel_efficientnet_landmarks.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Preprocessing of frame\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4429, 0.3043, 0.2806], \n",
    "                          std=[0.1187, 0.0874, 0.0728]),])\n",
    "\n",
    "# Open camera feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the frame counter and the start time for FPS\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a Mediapipe Hands and Face Detection objects\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# Create a Mediapipe Hands and Face Detection objects\n",
    "\n",
    "hands = mp_hands.Hands()\n",
    "face_detection = mp_face_detection.FaceDetection()\n",
    "\n",
    "\n",
    "# Loop through each frame in the video stream\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    start_time = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Flip the frame horizontally for a mirror-like effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect hands\n",
    "    hands_results = hands.process(frame_rgb)\n",
    "\n",
    "    # Detect faces\n",
    "    face_detection_results = face_detection.process(frame_rgb)\n",
    "\n",
    "    # Draw landmarks for hands\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Draw landmarks for faces\n",
    "    if face_detection_results.detections:\n",
    "        for detection in face_detection_results.detections:\n",
    "            mp.solutions.drawing_utils.draw_detection(\n",
    "                frame,\n",
    "                detection)\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Preprocess the image\n",
    "    pil_image = Image.fromarray(frame)\n",
    "    image_tensor = preprocess(pil_image)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move the data to the GPU\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Pass the image through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "\n",
    "    # Move the output to the CPU\n",
    "    output = output.cpu()\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    predicted_class = predicted.item()\n",
    "\n",
    "    # Define the class names\n",
    "    class_names = ['afraid',  'alone', 'boss', 'hello', 'tough']\n",
    "\n",
    "    # Display the predicted class on the frame\n",
    "    predicted_class_name = class_names[predicted_class]\n",
    "    stop_time = time.time()\n",
    "    cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, \"EffecientNet with landmarks\", (10, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"{round((stop_time - start_time) * 1000,3)} ms\", (460, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('frame', frame)\n",
    "    \n",
    "    # Exit if the user presses the 'q' key\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "\n",
    "# Release the VideoCapture object and close the OpenCV window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
