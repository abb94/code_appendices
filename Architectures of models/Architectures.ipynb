{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a3bbf3-d5e8-4408-afaf-75f83ba87ad9",
   "metadata": {},
   "source": [
    "# Architectures used for experiments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4affe-bb34-4338-8db7-9ad2adcd68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0ebbe-e3df-4d93-ad51-5c2fbed38267",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EffecientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4dbee-d40e-42fe-818b-5e499f47add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet_model(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=256, num_layers=2, dropout=0.25):\n",
    "        super().__init__()\n",
    "        # Load pre-trained model\n",
    "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        # Freeze parameters of the pre-trained model\n",
    "        for param in self.efficientnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Temporal feature extraction through BI-LSTM\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Dropout to prevent overfitting\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 256)\n",
    "        # Dropout to prevent overfitting\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.relu2 = nn.Relu()\n",
    "        # Last fully connected layer with number of classes as units to make predictions\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.efficientnet.extract_features(x)\n",
    "        # Flatten the feature maps and add a batch dimension\n",
    "        x = x.flatten(start_dim=2).permute(0, 2, 1)\n",
    "        # Pass the flattened feature maps through the bidirectional LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.relu(x)\n",
    "        # Concatenate the outputs of the forward and backward directions and pass through the fully connected layers with dropout\n",
    "        x = self.dropout1(torch.cat((x[:, -1, :self.lstm.hidden_size], x[:, 0, self.lstm.hidden_size:]), dim=1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0968031-90dd-40c8-bc21-c8c9847aad14",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc3649-f40a-4bc9-a7be-ccc99052c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_model(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=512, num_layers=2, dropout=0.5):\n",
    "        super(ResNetWithLSTM, self).__init__()\n",
    "        # Load pre-trained ResNet-18 model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        # Freeze all layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Last layer in ResNet18 is 1000 units, this is reduced to 512 units to fit BI-LSTM layer. \n",
    "        self.fc1 = nn.Linear(1000, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Dropout to reduce overtfitting\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # BI-LSTM layer to capture temporal feature extraction\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # Dropout to fight overfitting\n",
    "        self.dropout2 = nn.Dropout(dropout) \n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Last fully connected layer with number of classes\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)  # Apply dropout after the first fully connected layer\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout2(x)  # Apply dropout after the LSTM layer\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bcb56-ff79-4ff7-a56f-ef2a30c89916",
   "metadata": {},
   "source": [
    "## MobileNet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925cf526-0526-4bf0-8894-0bce0246fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNet_model(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=256, num_layers=2, dropout=0.25):\n",
    "        super().__init__()\n",
    "        #Load pre-trained model\n",
    "        self.mobilenet = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "        # Freeze weights of pre-trained model\n",
    "        for param in self.mobilenet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Adaptive layer to apply LSTM or Fully Connected layer    \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # BI-LSTM for temporal feature extraction\n",
    "        self.lstm = nn.LSTM(input_size=1280, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        # Dropout to reduce overfitting\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 512)\n",
    "        # Additional dropout to reduce overfitting\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        # Last fully connected layer to produce final predictions\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        # Softmax function\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mobilenet(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.expand(-1, self.lstm.num_layers * 2, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout1(torch.cat((x[:, -1, :self.lstm.hidden_size], x[:, 0, self.lstm.hidden_size:]), dim=1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b296b-b5dc-49c7-b0e2-011de2b67d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
